{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "assert (\n",
    "    os.name != \"nt\"\n",
    "), \"Windows is not supported, due to Triton not having a wheel for Windows. Please use Linux or macOS.\"\n",
    "\n",
    "import datasets\n",
    "from dotenv import load_dotenv\n",
    "from transformers import DataCollatorForSeq2Seq, TrainingArguments, pipeline\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "assert (\n",
    "    \"HF_TOKEN\" in os.environ\n",
    "), \"HF_TOKEN is not set. Use os.environ['HF_TOKEN'] = '...' to set it, or put it in .env file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_arguments():\n",
    "    args = argparse.Namespace()\n",
    "\n",
    "    args.max_seq_length = 2048\n",
    "    args.dtype = None\n",
    "    args.load_in_4bit = True\n",
    "\n",
    "    # More models at https://huggingface.co/unsloth\n",
    "    args.model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "    args.chat_template = \"llama-3.1\"\n",
    "\n",
    "    # PEFT parameters\n",
    "    args.peft_rank = 16\n",
    "    args.lora_alpha = 16\n",
    "    args.lora_dropout = 0\n",
    "\n",
    "    # Prompt\n",
    "    args.replacement_policy_system_prompt = \"You are a helpful assistant helping with CPU cache line eviction. You always output something from the cache lines list\"\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "args = get_default_arguments()\n",
    "print(f\"Using arguments: {args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=args.model_name, max_seq_length=args.max_seq_length, dtype=args.dtype, load_in_4bit=args.load_in_4bit\n",
    ")\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT - Parameter Efficient Fine-Tuning: https://huggingface.co/docs/peft\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=args.peft_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=args.lora_alpha,\n",
    "    lora_dropout=args.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cache_replacement.policy_learning.cache.eviction_policy import BeladyScorer, GreedyEvictionPolicy\n",
    "from cache_replacement.policy_learning.cache.memtrace import MemoryTrace\n",
    "from task13.cache import Cache\n",
    "\n",
    "cache_config = {\"cache_line_size\": 64, \"capacity\": 64 * 16 * 64, \"associativity\": 16}\n",
    "memtrace_path = os.path.join(os.getcwd(), \"data\", \"sample_trace.csv\")\n",
    "memtrace = MemoryTrace(memtrace_path, cache_line_size=cache_config[\"cache_line_size\"], max_look_ahead=int(1e5))\n",
    "\n",
    "# Initialize Belady's optimal eviction policy\n",
    "belady_scorer = BeladyScorer(memtrace)\n",
    "belady_policy = GreedyEvictionPolicy(belady_scorer)\n",
    "\n",
    "cache = Cache.from_config(cache_config, eviction_policy=belady_policy)\n",
    "\n",
    "\n",
    "class BeladyObserver:\n",
    "    def __init__(self):\n",
    "        self.last_cache_lines = list()\n",
    "\n",
    "    def __call__(self, cache_access, eviction_decision):\n",
    "        cache_lines = [hex(line) for (line, _) in cache_access.cache_lines]\n",
    "        self.last_cache_lines = cache_lines\n",
    "\n",
    "\n",
    "belady_observer = BeladyObserver()\n",
    "\n",
    "dataset = list()\n",
    "\n",
    "\n",
    "def get_prompt(pc, address, cache_lines):\n",
    "    prompt = \"\"\n",
    "    prompt += f\"Current PC is {hex(pc)}\\n\"\n",
    "    prompt += f\"Current address: {hex(address)}\\n\"\n",
    "    prompt += f\"Cache lines are: {cache_lines}\\n\"\n",
    "    prompt += \"Eviction: \"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_datapoint(pc, address, last_cache_lines, belady_eviction):\n",
    "    system_prompt_data = {\"role\": \"system\", \"content\": args.replacement_policy_system_prompt}\n",
    "\n",
    "    user_prompt_data = {\"role\": \"user\", \"content\": get_prompt(pc, address, last_cache_lines)}\n",
    "\n",
    "    answer_data = {\"role\": \"assistant\", \"content\": f\"{belady_eviction}\"}\n",
    "\n",
    "    return [system_prompt_data, user_prompt_data, answer_data]\n",
    "\n",
    "\n",
    "with memtrace:\n",
    "    while not memtrace.done():\n",
    "        pc, address = memtrace.next()\n",
    "        cache.read(pc, address, observers=[belady_observer])\n",
    "\n",
    "        belady_eviction = cache.last_evicted_cache_line\n",
    "        if belady_eviction is not None:\n",
    "            prompt = get_prompt(pc, address, belady_observer.last_cache_lines)\n",
    "            assert str(hex(belady_eviction)) in belady_observer.last_cache_lines\n",
    "            dataset.append(generate_datapoint(pc, address, belady_observer.last_cache_lines, str(hex(belady_eviction))))\n",
    "hit_rate = cache.hit_rate_statistic.success_rate()\n",
    "print(hit_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = datasets.Dataset.from_dict({\"conversations\": dataset})\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }\n",
    "\n",
    "\n",
    "hf_dataset = hf_dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The following is an example of an input prompt to the model:\\n\")\n",
    "print(hf_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.for_training(model)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=hf_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=args.max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    dataset_num_proc=8,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps=60,\n",
    "        learning_rate=1e-5,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",  # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training\")\n",
    "t_start = time.time()\n",
    "trainer.train()\n",
    "t_end = time.time()\n",
    "print(f\"Training complete in {t_end - t_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_input_prompt(input_tokens, output_tokens):\n",
    "    input_squeezed = input_tokens.squeeze()\n",
    "    output_squeezed = output_tokens.squeeze()\n",
    "    assert len(input_squeezed) < len(output_squeezed)\n",
    "    return output_squeezed[len(input_squeezed) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": args.replacement_policy_system_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Current PC is 0x10\\nCurrent address: 0x1000\\nCache lines are: ['0x1e244', '0x1e23', 0x13]\\nEviction: \",\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "out = model.generate(input_ids=inputs, max_new_tokens=1024, use_cache=True, temperature=1.4, min_p=0.1, do_sample=True)\n",
    "\n",
    "decoded = tokenizer.decode(skip_input_prompt(inputs, out).cpu().numpy(), skip_special_tokens=True)\n",
    "print(\"LLM output:\\n\\n\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant helping with CPU cache line policy improvement\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"So far, you only get PC, current address, cache lines. How do we combine them to generate a new feature for better line eviction predictor? Give us just one, it's a feature engineering task\",\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "out = model.generate(input_ids=inputs, max_new_tokens=1024, use_cache=True, temperature=0.3, min_p=0.1, do_sample=True)\n",
    "\n",
    "decoded = tokenizer.decode(skip_input_prompt(inputs, out).cpu().numpy(), skip_special_tokens=True)\n",
    "print(\"LLM output:\\n\\n\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model\n",
    "store_path = os.path.join(os.getcwd(), \"outputs\", \"llama-3.2-3b-instruct\")\n",
    "model.save_pretrained(store_path)\n",
    "tokenizer.save_pretrained(store_path)\n",
    "print(f\"Model stored in {store_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "load_path = os.path.join(os.getcwd(), \"outputs\", \"llama-3.2-3b-instruct\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(load_path)\n",
    "print(f\"Model loaded from {load_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model with a pipeline\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=500)\n",
    "print(pipe(\"What prompt should I put here?\", return_full_text=False)[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
