{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "load_path = os.path.join(os.getcwd(), \"outputs\", \"checkpoint-0\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(load_path)\n",
    "print(f\"Model loaded from {load_path}\")\n",
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Prompt = namedtuple(\"Prompt\", [\"system\", \"user\"])\n",
    "\n",
    "prompts = [\n",
    "    Prompt(\n",
    "        system=\"You are a helpful assistant, answering user questions about computer-architecture related topics\",\n",
    "        user=\"What is the PC??\"\n",
    "        ),\n",
    "    Prompt(\n",
    "        system=\"You are a helpful assistant helping with CPU cache line eviction.\",\n",
    "        user=\"What features were used during your fine-tuning? There are only 4. Pick from [Current PC, Current address, Cache lines, Cache size, Cache Miss Rate, Cache Hit Rate]\"\n",
    "        ),\n",
    "    \n",
    "    Prompt(\n",
    "        system=\"You are a helpful assistant helping with CPU cache line eviction.\",\n",
    "        user=\"\"\"\n",
    "            This is our current prompt to an eviction policy.\n",
    "            What can we changes should be made to help the model better identify the cache line to evict and ultimately make the eviction policy more efficient?\n",
    "            \n",
    "            Prompt: \n",
    "            Current PC is <pc>\n",
    "            Current address: <list-of-addresses>\n",
    "            Cache lines are: <list-of-cache-lines>\n",
    "            Eviction:\n",
    "            \"\"\"\n",
    "        ),\n",
    "    \n",
    "]\n",
    "\n",
    "def evaluate(checkpoint, prompt):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(checkpoint)\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt.system}, {\"role\": \"user\", \"content\": prompt.user}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,  # Must add for generation\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    out = model.generate(\n",
    "        input_ids=inputs, max_new_tokens=1024, use_cache=True, temperature=0.3, min_p=0.1, do_sample=False\n",
    "    )\n",
    "    decoded = tokenizer.decode(out.squeeze()[inputs.shape[1]:].cpu().numpy(), skip_special_tokens=True)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = [os.path.join(os.getcwd(), \"outputs\", f\"checkpoint-{idx}\") for idx in [0, 1000, 2000, 3000, 4000, 5000, 6000]]\n",
    "\n",
    "prompt_idx = 2\n",
    "results = list()\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    answer = evaluate(checkpoint, prompts[prompt_idx])\n",
    "    results.append(\n",
    "        (checkpoint, answer)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cache_hw03]",
   "language": "python",
   "name": "conda-env-cache_hw03-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
