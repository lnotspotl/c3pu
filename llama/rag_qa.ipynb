{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "from pandasai import SmartDataframe\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_documents(documents: Union[List[str], str]) -> List[Document]:\n",
    "    if isinstance(documents, str):\n",
    "        ext = {\"txt\" : TextLoader, \"pdf\" : PyPDFLoader}\n",
    "        extension = documents.split(\".\")[-1]\n",
    "\n",
    "        if extension not in ext:\n",
    "            raise ValueError(f\"Unsupported extension: .{documents.split()[-1]}. Supported document extensions are {list(ext.keys())}\")\n",
    "        \n",
    "        loader = ext[extension](documents)\n",
    "        return loader.load()\n",
    "\n",
    "    if isinstance(documents, list):\n",
    "        loaded = list()\n",
    "        for item in documents:\n",
    "            assert isinstance(item, str), \"Expected document paths to be of type str\"\n",
    "            loaded.extend(load_documents(item))\n",
    "        return loaded\n",
    "\n",
    "    raise ValueError(f\"Unknown type of documents: {type(documents)}. Supported types are str and List[str]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_architecture_book():\n",
    "    filename = os.path.join(\"data\", \"arch.pdf\")\n",
    "    \n",
    "    # Download nothing if file already exists\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File {filename} already exists. Downloading nothing!\")\n",
    "        return\n",
    "\n",
    "    # Download file because it does not exist\n",
    "    import subprocess\n",
    "    downloaded_name = \"Computer Architecture A Quantitative Approach (5th edition).pdf\"\n",
    "    if not os.path.exists(downloaded_name):\n",
    "        url = \"https://acs.pub.ro/~cpop/SMPA/Computer%20Architecture%20A%20Quantitative%20Approach%20(5th%20edition).pdf\"\n",
    "        subprocess.run([\"wget\", \"--no-check-certificate\", url])\n",
    "    os.rename(downloaded_name, filename)\n",
    "\n",
    "download_architecture_book()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model from finetune.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "load_path = os.path.join(os.getcwd(), \"outputs\", \"llama-3.2-3b-instruct\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(load_path)\n",
    "print(f\"Model loaded from {load_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simplest of RAG systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RaggenQAConfig:\n",
    "    k: int = 5\n",
    "    embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "class RaggenQA:\n",
    "    def __init__(self, model, tokenizer, documents, config: RaggenQAConfig):\n",
    "        self.config = config\n",
    "        self.vector_store = self._create_vector_store(documents)\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _create_vector_store(self, documents):\n",
    "        documents = load_documents(documents)\n",
    "\n",
    "        # Split documents into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "        )\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Create embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=self.config.embedding_model_name)\n",
    "\n",
    "        # Create vector store\n",
    "        vector_store = FAISS.from_documents(texts, embeddings)\n",
    "        return vector_store\n",
    "\n",
    "    def _skip_input_prompt(self, input_tokens, output_tokens):\n",
    "        input_squeezed = input_tokens.squeeze()\n",
    "        output_squeezed = output_tokens.squeeze()\n",
    "        assert len(input_squeezed) < len(output_squeezed)\n",
    "        return output_squeezed[len(input_squeezed) :]\n",
    "    \n",
    "    def ask(self, question):\n",
    "        model = FastLanguageModel.for_inference(self.model)\n",
    "\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "            You are a helpful QA assistant, answering user questions based on pieces of context from documents.\n",
    "            Use the following pieces of context to answer the question at the end.\n",
    "            If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        \"\"\"\n",
    "\n",
    "        for document in self.vector_store.similarity_search(question, k=self.config.k):\n",
    "            SYSTEM_PROMPT += f\"\\n{document.page_content}\\n\"\n",
    "\n",
    "        USER_PROMPT = question\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": USER_PROMPT}]\n",
    "\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,  # Must add for generation\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        out = model.generate(\n",
    "            input_ids=inputs, max_new_tokens=1024, use_cache=True, temperature=0.3, min_p=0.1, do_sample=True\n",
    "        )\n",
    "\n",
    "        decoded = tokenizer.decode(self._skip_input_prompt(inputs, out).cpu().numpy(), skip_special_tokens=True)\n",
    "\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raggen_qa = RaggenQA(model, tokenizer, os.path.join(\"data\", \"raggen_1000.txt\"), RaggenQAConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What features can be used for a good cache eviction policy?\"\n",
    "answer = raggen_qa.ask(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self-consistency RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RaggenQAConfigSC:\n",
    "    k: int = 3\n",
    "    embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    self_consistency_k: int = 3\n",
    "    self_consistency_temperature: float = 0.3\n",
    "\n",
    "class RaggenQASC:\n",
    "    def __init__(self, model, tokenizer, document, config: RaggenQAConfig):\n",
    "        self.config = config\n",
    "        self.vector_store = self._create_vector_store(document)\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _create_vector_store(self, document):\n",
    "        # Load documents\n",
    "        loader = TextLoader(document)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Split documents into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "        )\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Create embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=self.config.embedding_model_name)\n",
    "\n",
    "        # Create vector store\n",
    "        vector_store = FAISS.from_documents(texts, embeddings)\n",
    "        return vector_store\n",
    "\n",
    "    def _skip_input_prompt(self, input_tokens, output_tokens):\n",
    "        input_squeezed = input_tokens.squeeze()\n",
    "        output_squeezed = output_tokens.squeeze()\n",
    "        assert len(input_squeezed) < len(output_squeezed)\n",
    "        return output_squeezed[len(input_squeezed) :]\n",
    "    \n",
    "    def ask(self, question):\n",
    "        model = FastLanguageModel.for_inference(self.model)\n",
    "\n",
    "        answers = list()\n",
    "\n",
    "        # Self-consistency\n",
    "        for _ in range(self.config.self_consistency_k):\n",
    "\n",
    "            SYSTEM_PROMPT = \"\"\"\n",
    "                You are a helpful QA assistant, answering user questions based on pieces of context from documents.\n",
    "                Use the following pieces of context to answer the question at the end.\n",
    "                If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "            \"\"\"\n",
    "\n",
    "            for document in self.vector_store.similarity_search(question, k=self.config.k):\n",
    "                SYSTEM_PROMPT += f\"\\n{document.page_content}\\n\"\n",
    "\n",
    "            USER_PROMPT = question\n",
    "\n",
    "            messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": USER_PROMPT}]\n",
    "\n",
    "            inputs = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,  # Must add for generation\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            out = model.generate(\n",
    "                input_ids=inputs, max_new_tokens=1024, use_cache=True, temperature=self.config.self_consistency_temperature, min_p=0.1, do_sample=True\n",
    "            )\n",
    "\n",
    "            decoded = tokenizer.decode(self._skip_input_prompt(inputs, out).cpu().numpy(), skip_special_tokens=True)\n",
    "\n",
    "            answers.append(decoded)\n",
    "\n",
    "        # Majority vote\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "            You are a helpful QA assistant using self-consistency, answering user questions based on pieces of context from documents.\n",
    "            Use the following pieces of context to answer the question at the end.\n",
    "            If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        \"\"\"\n",
    "\n",
    "        for document in self.vector_store.similarity_search(question, k=self.config.k):\n",
    "            SYSTEM_PROMPT += f\"\\n{document.page_content}\\n\"\n",
    "\n",
    "        SYSTEM_PROMPT += f\"Your previous {self.config.self_consistency_k} answers were:\\n\\n\"\n",
    "        SYSTEM_PROMPT += \"\\n\\n\".join(answers)\n",
    "\n",
    "        USER_PROMPT = f\"Based on your previous {self.config.self_consistency_k} answers, what is your final answer to the original question: {question}?\"\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": USER_PROMPT}]\n",
    "\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,  # Must add for generation\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        out = model.generate(\n",
    "            input_ids=inputs, max_new_tokens=1024, use_cache=True, temperature=0.2, min_p=0.1, do_sample=True\n",
    "        )\n",
    "\n",
    "        decoded = tokenizer.decode(self._skip_input_prompt(inputs, out).cpu().numpy(), skip_special_tokens=True)\n",
    "\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raggen_qa = RaggenQASC(model, tokenizer, os.path.join(\"data\", \"raggen_1000.txt\"), RaggenQAConfigSC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What line did LRU evict when the PC was 0x413a4b?\"\n",
    "answer = raggen_qa.ask(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table Question-Answering system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai.llm.base import LLM, BaseOpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Optional, List, Any\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from pandasai.prompts.base import BasePrompt\n",
    "from pandasai.pipelines.pipeline_context import PipelineContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceLangChainLLM(LLM):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        return \"local\"\n",
    "\n",
    "    def call(self, instruction: BasePrompt, context: PipelineContext = None):\n",
    "        model = FastLanguageModel.for_inference(self.model)\n",
    "        \n",
    "        self.last_prompt = str(instruction)\n",
    "        memory = context.memory if context else None\n",
    "        prompt = self.prepend_system_prompt(self.last_prompt, memory)\n",
    "\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "            You are a helpful assistant, that, if asked to generate python code, only generates python code and nothing else, no even explanations.\n",
    "            When generating code, including nothing else, for example:\n",
    "        \"\"\"\n",
    "        USER_PROMPT = prompt\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "        ]\n",
    "\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        out = model.generate(input_ids = inputs, max_new_tokens = 1024,\n",
    "                           use_cache = True, temperature = 0.3, min_p = 0.1, do_sample=False)\n",
    "\n",
    "        decoded = tokenizer.decode(self._skip_input_prompt(inputs, out).cpu().numpy(), skip_special_tokens=True)\n",
    "        \n",
    "        ## !! HACK !! TODO: (lnotspotl) Figure out how we can get rid of this\n",
    "        decoded = decoded.strip()\n",
    "        if(decoded.startswith(\"```python\")):\n",
    "            decoded = decoded[len(\"```python\"):]\n",
    "        if decoded.endswith(\"```\"):\n",
    "            decoded = decoded[:-3]\n",
    "        return decoded\n",
    "\n",
    "    def _skip_input_prompt(self, input_tokens, output_tokens):\n",
    "        input_squeezed = input_tokens.squeeze()\n",
    "        output_squeezed = output_tokens.squeeze()\n",
    "        assert len(input_squeezed) < len(output_squeezed)\n",
    "        return output_squeezed[len(input_squeezed):] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load default llama model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-3B-Instruct\", max_seq_length=2048, dtype=None, load_in_4bit=True\n",
    ")\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into memory\n",
    "df = pd.read_csv(os.path.join(\"data\", \"llc_access_trace_features.csv\"))\n",
    "display(df.head())\n",
    "\n",
    "# Create smart dataframe\n",
    "llm = HuggingFaceLangChainLLM(model, tokenizer)\n",
    "smart_df = SmartDataframe(df, config={\"llm\": llm})\n",
    "\n",
    "# Ask question\n",
    "question = \"What is the average value of recency?\"\n",
    "answer = smart_df.chat(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cache_hw03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
