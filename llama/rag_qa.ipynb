{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "load_path = os.path.join(os.getcwd(), \"outputs\", \"llama-3.2-3b-instruct\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(load_path)\n",
    "print(f\"Model loaded from {load_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaggenQA:\n",
    "    def __init__(self, model, tokenizer, document):\n",
    "        self.embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        self.vector_store = self._create_vector_store(document)\n",
    "        self.k = 3\n",
    "        \n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _create_vector_store(self, document):\n",
    "        # Load documents\n",
    "        loader = TextLoader(document)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "        )\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Create embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=self.embedding_model_name\n",
    "        )\n",
    "        \n",
    "        # Create vector store\n",
    "        vector_store = FAISS.from_documents(texts, embeddings)\n",
    "        return vector_store\n",
    "\n",
    "    def _skip_input_prompt(self, input_tokens, output_tokens):\n",
    "        input_squeezed = input_tokens.squeeze()\n",
    "        output_squeezed = output_tokens.squeeze()\n",
    "        assert len(input_squeezed) < len(output_squeezed)\n",
    "        return output_squeezed[len(input_squeezed):] \n",
    "\n",
    "    def ask(self, question):\n",
    "        model = FastLanguageModel.for_inference(self.model)\n",
    "\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "            You are a helpful QA assistant, answering user questions based on pieces of context from documents.\n",
    "            Use the following pieces of context to answer the question at the end.\n",
    "            If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        \"\"\"\n",
    "\n",
    "        for document in self.vector_store.similarity_search(question, k=self.k):\n",
    "            SYSTEM_PROMPT += f\"\\n{document.page_content}\\n\"\n",
    "\n",
    "        USER_PROMPT = question\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "        ]\n",
    "\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        out = model.generate(input_ids = inputs, max_new_tokens = 1024,\n",
    "                           use_cache = True, temperature = 0.3, min_p = 0.1, do_sample=True)\n",
    "\n",
    "        decoded = tokenizer.decode(self._skip_input_prompt(inputs, out).cpu().numpy(), skip_special_tokens=True)\n",
    "\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raggen_qa = RaggenQA(model, tokenizer, \"text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raggen_qa.ask(\"What is the capital of France?\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
